{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scalable Exact GP Posterior Sampling using Contour Integral Quadrature\n",
    "\n",
    "This notebook demonstrates the most simple usage of contour integral quadrature with msMINRES as described [here](https://arxiv.org/pdf/2006.11267.pdf) to sample from the predictive distribution of an exact GP.\n",
    "\n",
    "Note that to achieve results where Cholesky would run the GPU out of memory, you'll need to have KeOps installed (see our KeOps tutorial in this same folder). Despite this, on this relatively simple example with 1000 training points but seeing to sample at 20000 test points in 1D, we will achieve significant speed ups over Cholesky."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import torch\n",
    "import gpytorch\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\", gpytorch.utils.warnings.NumericalWarning)\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training data is 11 points in [0,1] inclusive regularly spaced\n",
    "train_x = torch.linspace(0, 1, 1000)\n",
    "# True function is sin(2*pi*x) with Gaussian noise\n",
    "train_y = torch.sin(train_x * (2 * math.pi)) + torch.randn(train_x.size()) * 0.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Are we running with KeOps?\n",
    "\n",
    "If you have KeOps, change the below flag to `True` to run with a significantly larger test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "HAVE_KEOPS = False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define an Exact GP Model and train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExactGPModel(gpytorch.models.ExactGP):\n",
    "    def __init__(self, train_x, train_y, likelihood):\n",
    "        super(ExactGPModel, self).__init__(train_x, train_y, likelihood)\n",
    "        self.mean_module = gpytorch.means.ConstantMean()\n",
    "        \n",
    "        if HAVE_KEOPS:\n",
    "            self.covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.keops.RBFKernel())\n",
    "        else:\n",
    "            self.covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel())\n",
    "    \n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n",
    "\n",
    "# initialize likelihood and model\n",
    "likelihood = gpytorch.likelihoods.GaussianLikelihood()\n",
    "model = ExactGPModel(train_x, train_y, likelihood)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    train_x = train_x.cuda()\n",
    "    train_y = train_y.cuda()\n",
    "    model = model.cuda()\n",
    "    likelihood = likelihood.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 1/50 - Loss: 0.862   lengthscale: 0.693   noise: 0.693\n",
      "Iter 2/50 - Loss: 0.818   lengthscale: 0.644   noise: 0.644\n",
      "Iter 3/50 - Loss: 0.766   lengthscale: 0.598   noise: 0.598\n",
      "Iter 4/50 - Loss: 0.716   lengthscale: 0.554   noise: 0.554\n",
      "Iter 5/50 - Loss: 0.664   lengthscale: 0.513   noise: 0.513\n",
      "Iter 6/50 - Loss: 0.622   lengthscale: 0.474   noise: 0.474\n",
      "Iter 7/50 - Loss: 0.579   lengthscale: 0.439   noise: 0.437\n",
      "Iter 8/50 - Loss: 0.533   lengthscale: 0.408   noise: 0.402\n",
      "Iter 9/50 - Loss: 0.489   lengthscale: 0.380   noise: 0.370\n",
      "Iter 10/50 - Loss: 0.448   lengthscale: 0.356   noise: 0.339\n",
      "Iter 11/50 - Loss: 0.418   lengthscale: 0.335   noise: 0.311\n",
      "Iter 12/50 - Loss: 0.377   lengthscale: 0.317   noise: 0.285\n",
      "Iter 13/50 - Loss: 0.340   lengthscale: 0.301   noise: 0.261\n",
      "Iter 14/50 - Loss: 0.303   lengthscale: 0.287   noise: 0.238\n",
      "Iter 15/50 - Loss: 0.267   lengthscale: 0.275   noise: 0.217\n",
      "Iter 16/50 - Loss: 0.232   lengthscale: 0.265   noise: 0.198\n",
      "Iter 17/50 - Loss: 0.199   lengthscale: 0.256   noise: 0.181\n",
      "Iter 18/50 - Loss: 0.158   lengthscale: 0.248   noise: 0.164\n",
      "Iter 19/50 - Loss: 0.122   lengthscale: 0.242   noise: 0.150\n",
      "Iter 20/50 - Loss: 0.091   lengthscale: 0.237   noise: 0.136\n",
      "Iter 21/50 - Loss: 0.050   lengthscale: 0.232   noise: 0.124\n",
      "Iter 22/50 - Loss: 0.020   lengthscale: 0.228   noise: 0.113\n",
      "Iter 23/50 - Loss: -0.005   lengthscale: 0.225   noise: 0.103\n",
      "Iter 24/50 - Loss: -0.032   lengthscale: 0.222   noise: 0.094\n",
      "Iter 25/50 - Loss: -0.057   lengthscale: 0.220   noise: 0.085\n",
      "Iter 26/50 - Loss: -0.074   lengthscale: 0.219   noise: 0.078\n",
      "Iter 27/50 - Loss: -0.101   lengthscale: 0.218   noise: 0.071\n",
      "Iter 28/50 - Loss: -0.118   lengthscale: 0.218   noise: 0.065\n",
      "Iter 29/50 - Loss: -0.137   lengthscale: 0.218   noise: 0.060\n",
      "Iter 30/50 - Loss: -0.153   lengthscale: 0.219   noise: 0.055\n",
      "Iter 31/50 - Loss: -0.161   lengthscale: 0.220   noise: 0.051\n",
      "Iter 32/50 - Loss: -0.167   lengthscale: 0.222   noise: 0.047\n",
      "Iter 33/50 - Loss: -0.179   lengthscale: 0.223   noise: 0.044\n",
      "Iter 34/50 - Loss: -0.180   lengthscale: 0.225   noise: 0.041\n",
      "Iter 35/50 - Loss: -0.170   lengthscale: 0.227   noise: 0.038\n",
      "Iter 36/50 - Loss: -0.184   lengthscale: 0.229   noise: 0.036\n",
      "Iter 37/50 - Loss: -0.167   lengthscale: 0.232   noise: 0.035\n",
      "Iter 38/50 - Loss: -0.174   lengthscale: 0.235   noise: 0.033\n",
      "Iter 39/50 - Loss: -0.169   lengthscale: 0.238   noise: 0.032\n",
      "Iter 40/50 - Loss: -0.164   lengthscale: 0.240   noise: 0.031\n",
      "Iter 41/50 - Loss: -0.156   lengthscale: 0.244   noise: 0.030\n",
      "Iter 42/50 - Loss: -0.168   lengthscale: 0.247   noise: 0.030\n",
      "Iter 43/50 - Loss: -0.155   lengthscale: 0.251   noise: 0.030\n",
      "Iter 44/50 - Loss: -0.165   lengthscale: 0.254   noise: 0.029\n",
      "Iter 45/50 - Loss: -0.160   lengthscale: 0.258   noise: 0.030\n",
      "Iter 46/50 - Loss: -0.161   lengthscale: 0.263   noise: 0.030\n",
      "Iter 47/50 - Loss: -0.166   lengthscale: 0.267   noise: 0.030\n",
      "Iter 48/50 - Loss: -0.166   lengthscale: 0.271   noise: 0.031\n",
      "Iter 49/50 - Loss: -0.167   lengthscale: 0.276   noise: 0.031\n",
      "Iter 50/50 - Loss: -0.175   lengthscale: 0.279   noise: 0.032\n"
     ]
    }
   ],
   "source": [
    "# this is for running the notebook in our testing framework\n",
    "import os\n",
    "smoke_test = ('CI' in os.environ)\n",
    "training_iter = 2 if smoke_test else 50\n",
    "\n",
    "# Find optimal model hyperparameters\n",
    "model.train()\n",
    "likelihood.train()\n",
    "\n",
    "# Use the adam optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.1)  # Includes GaussianLikelihood parameters\n",
    "\n",
    "# \"Loss\" for GPs - the marginal log likelihood\n",
    "mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)\n",
    "\n",
    "for i in range(training_iter):\n",
    "    # Zero gradients from previous iteration\n",
    "    optimizer.zero_grad()\n",
    "    # Output from model\n",
    "    output = model(train_x)\n",
    "    # Calc loss and backprop gradients\n",
    "    loss = -mll(output, train_y)\n",
    "    loss.backward()\n",
    "    print('Iter %d/%d - Loss: %.3f   lengthscale: %.3f   noise: %.3f' % (\n",
    "        i + 1, training_iter, loss.item(),\n",
    "        model.covar_module.base_kernel.lengthscale.item(),\n",
    "        model.likelihood.noise.item()\n",
    "    ))\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define test set\n",
    "\n",
    "If we have KeOps installed, we'll test on 50000 points instead of 10000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([50000])\n"
     ]
    }
   ],
   "source": [
    "if HAVE_KEOPS:\n",
    "    test_n = 50000\n",
    "else:\n",
    "    test_n = 10000\n",
    "\n",
    "test_x = torch.linspace(0, 1, test_n)\n",
    "if torch.cuda.is_available():\n",
    "    test_x = test_x.cuda()\n",
    "print(test_x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Draw a sample with CIQ\n",
    "\n",
    "To do this, we just add the `ciq_samples` setting to the rsample call. We additionally demonstrate all relevant settings for controlling Contour Integral Quadrature:\n",
    "\n",
    "- The `ciq_samples` setting determines whether or not to use CIQ\n",
    "- The `num_contour_quadrature` setting controls the number of quadrature sites (Q in the paper).\n",
    "- The `minres_tolerance` setting controls the error we tolerate from minres (here, <0.01%).\n",
    "\n",
    "Note that, of these settings, increase num_contour_quadrature is unlikely to improve performance. As Theorem 1 from the paper demonstrates, virtually all of the error in this method is controlled by minres_tolerance. Here, we use a quite tight tolerance for minres."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running with CIQ\n",
      "CPU times: user 3min 22s, sys: 18.4 s, total: 3min 40s\n",
      "Wall time: 11.9 s\n",
      "Running with Cholesky\n",
      "\n",
      "Intel MKL ERROR: Parameter 8 was incorrect on entry to DSYEVD.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "false INTERNAL ASSERT FAILED at \"../aten/src/ATen/native/BatchLinearAlgebra.cpp\":1539, please report a bug to PyTorch. linalg.eigh: Argument 8 has illegal value. Most certainly there is a bug in the implementation calling the backend library.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[0;32m<timed exec>:1\u001b[0m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/gpytorch/distributions/multivariate_normal.py:227\u001b[0m, in \u001b[0;36mMultivariateNormal.rsample\u001b[0;34m(self, sample_shape, base_samples)\u001b[0m\n\u001b[1;32m    224\u001b[0m     num_samples \u001b[38;5;241m=\u001b[39m sample_shape\u001b[38;5;241m.\u001b[39mnumel() \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    226\u001b[0m     \u001b[38;5;66;03m# Get samples\u001b[39;00m\n\u001b[0;32m--> 227\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[43mcovar\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzero_mean_mvn_samples\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_samples\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloc\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m    228\u001b[0m     res \u001b[38;5;241m=\u001b[39m res\u001b[38;5;241m.\u001b[39mview(sample_shape \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloc\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m    230\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/linear_operator/operators/_linear_operator.py:2733\u001b[0m, in \u001b[0;36mLinearOperator.zero_mean_mvn_samples\u001b[0;34m(self, num_samples)\u001b[0m\n\u001b[1;32m   2731\u001b[0m     covar_root \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mto_dense()\u001b[38;5;241m.\u001b[39msqrt()\n\u001b[1;32m   2732\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2733\u001b[0m     covar_root \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mroot_decomposition\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mroot\n\u001b[1;32m   2735\u001b[0m base_samples \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn(\n\u001b[1;32m   2736\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_shape,\n\u001b[1;32m   2737\u001b[0m     covar_root\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2740\u001b[0m     device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice,\n\u001b[1;32m   2741\u001b[0m )\n\u001b[1;32m   2742\u001b[0m samples \u001b[38;5;241m=\u001b[39m covar_root\u001b[38;5;241m.\u001b[39mmatmul(base_samples)\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m))\u001b[38;5;241m.\u001b[39mcontiguous()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/linear_operator/utils/memoize.py:59\u001b[0m, in \u001b[0;36m_cached.<locals>.g\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     57\u001b[0m kwargs_pkl \u001b[38;5;241m=\u001b[39m pickle\u001b[38;5;241m.\u001b[39mdumps(kwargs)\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _is_in_cache(\u001b[38;5;28mself\u001b[39m, cache_name, \u001b[38;5;241m*\u001b[39margs, kwargs_pkl\u001b[38;5;241m=\u001b[39mkwargs_pkl):\n\u001b[0;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _add_to_cache(\u001b[38;5;28mself\u001b[39m, cache_name, \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;241m*\u001b[39margs, kwargs_pkl\u001b[38;5;241m=\u001b[39mkwargs_pkl)\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _get_from_cache(\u001b[38;5;28mself\u001b[39m, cache_name, \u001b[38;5;241m*\u001b[39margs, kwargs_pkl\u001b[38;5;241m=\u001b[39mkwargs_pkl)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/linear_operator/operators/_linear_operator.py:2166\u001b[0m, in \u001b[0;36mLinearOperator.root_decomposition\u001b[0;34m(self, method)\u001b[0m\n\u001b[1;32m   2162\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m RootLinearOperator(\n\u001b[1;32m   2163\u001b[0m         to_linear_operator(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mto_dense())\u001b[38;5;241m.\u001b[39mpivoted_cholesky(rank\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_root_decomposition_size())\n\u001b[1;32m   2164\u001b[0m     )\n\u001b[1;32m   2165\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m method \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msymeig\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m-> 2166\u001b[0m     evals, evecs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_symeig\u001b[49m\u001b[43m(\u001b[49m\u001b[43meigenvectors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m   2167\u001b[0m     \u001b[38;5;66;03m# TODO: only use non-zero evals (req. dealing w/ batches...)\u001b[39;00m\n\u001b[1;32m   2168\u001b[0m     root \u001b[38;5;241m=\u001b[39m evecs \u001b[38;5;241m*\u001b[39m evals\u001b[38;5;241m.\u001b[39mclamp_min(\u001b[38;5;241m0.0\u001b[39m)\u001b[38;5;241m.\u001b[39msqrt()\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/linear_operator/operators/added_diag_linear_operator.py:203\u001b[0m, in \u001b[0;36mAddedDiagLinearOperator._symeig\u001b[0;34m(self, eigenvectors, return_evals_as_lazy)\u001b[0m\n\u001b[1;32m    197\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_symeig\u001b[39m(\n\u001b[1;32m    198\u001b[0m     \u001b[38;5;28mself\u001b[39m: Float[LinearOperator, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*batch N N\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    199\u001b[0m     eigenvectors: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    200\u001b[0m     return_evals_as_lazy: Optional[\u001b[38;5;28mbool\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    201\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[Float[Tensor, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*batch M\u001b[39m\u001b[38;5;124m\"\u001b[39m], Optional[Float[LinearOperator, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*batch N M\u001b[39m\u001b[38;5;124m\"\u001b[39m]]]:\n\u001b[1;32m    202\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_diag_tensor, ConstantDiagLinearOperator):\n\u001b[0;32m--> 203\u001b[0m         evals_, evecs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_linear_op\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_symeig\u001b[49m\u001b[43m(\u001b[49m\u001b[43meigenvectors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meigenvectors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    204\u001b[0m         evals \u001b[38;5;241m=\u001b[39m evals_ \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_diag_tensor\u001b[38;5;241m.\u001b[39m_diagonal()\n\u001b[1;32m    205\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m evals, evecs\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/linear_operator/operators/_linear_operator.py:886\u001b[0m, in \u001b[0;36mLinearOperator._symeig\u001b[0;34m(self, eigenvectors, return_evals_as_lazy)\u001b[0m\n\u001b[1;32m    884\u001b[0m \u001b[38;5;66;03m# potentially perform decomposition in double precision for numerical stability\u001b[39;00m\n\u001b[1;32m    885\u001b[0m dtype \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdtype\n\u001b[0;32m--> 886\u001b[0m evals, evecs \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinalg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meigh\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_dense\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msettings\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_linalg_dtype_symeig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalue\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    887\u001b[0m \u001b[38;5;66;03m# chop any negative eigenvalues.\u001b[39;00m\n\u001b[1;32m    888\u001b[0m \u001b[38;5;66;03m# TODO: warn if evals are significantly negative\u001b[39;00m\n\u001b[1;32m    889\u001b[0m evals \u001b[38;5;241m=\u001b[39m evals\u001b[38;5;241m.\u001b[39mclamp_min(\u001b[38;5;241m0.0\u001b[39m)\u001b[38;5;241m.\u001b[39mto(dtype\u001b[38;5;241m=\u001b[39mdtype)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: false INTERNAL ASSERT FAILED at \"../aten/src/ATen/native/BatchLinearAlgebra.cpp\":1539, please report a bug to PyTorch. linalg.eigh: Argument 8 has illegal value. Most certainly there is a bug in the implementation calling the backend library."
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "model.train()\n",
    "likelihood.train()\n",
    "\n",
    "# Get into evaluation (predictive posterior) mode\n",
    "model.eval()\n",
    "likelihood.eval()\n",
    "\n",
    "# Test points are regularly spaced along [0,1]\n",
    "# Make predictions by feeding model through likelihood\n",
    "\n",
    "test_x.requires_grad_(True)\n",
    "\n",
    "with torch.no_grad():\n",
    "    observed_pred = likelihood(model(test_x))\n",
    "    \n",
    "    # All relevant settings for using CIQ.\n",
    "    #   ciq_samples(True) - Use CIQ for sampling\n",
    "    #   num_contour_quadrature(10) -- Use 10 quadrature sites (Q in the paper)\n",
    "    #   minres_tolerance -- error tolerance from minres (here, <0.01%).\n",
    "    print(\"Running with CIQ\")\n",
    "    with gpytorch.settings.ciq_samples(True), gpytorch.settings.num_contour_quadrature(10), gpytorch.settings.minres_tolerance(1e-4):\n",
    "        %time y_samples = observed_pred.rsample()\n",
    "    \n",
    "    print(\"Running with Cholesky\")\n",
    "    # Make sure we use Cholesky\n",
    "    with gpytorch.settings.fast_computations(covar_root_decomposition=False):\n",
    "        %time y_samples = observed_pred.rsample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
